{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import compute_sample_weight, compute_class_weight\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('../../datasets/credit_card_fraud/creditcard.csv')\n",
    "X = dataframe.iloc[:,:-1].values\n",
    "y = dataframe.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# (PART 2)\n",
    "# After we realized the class imbalance in the data (PART 2)\n",
    "# class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "# class_weights_dict = dict(zip(np.unique(y_train), class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This GradientBoostingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 56\u001b[0m\n\u001b[1;32m     50\u001b[0m model \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# model.fit(X_train, y_train)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m y_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]  \n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_proba:\u001b[39m\u001b[38;5;124m'\u001b[39m,y_proba)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Adjusting the threshold to optimize precision-recall trade-off\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:1333\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict class probabilities for X.\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \n\u001b[1;32m   1315\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;124;03m        If the ``loss`` does not support probabilities.\u001b[39;00m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1333\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X)\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss\u001b[38;5;241m.\u001b[39m_raw_prediction_to_proba(raw_predictions)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:1242\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the decision function of ``X``.\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m \n\u001b[1;32m   1223\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;124;03m    array of shape (n_samples,).\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1240\u001b[0m     X, dtype\u001b[38;5;241m=\u001b[39mDTYPE, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m )\n\u001b[0;32m-> 1242\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_predict(X)\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_predictions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m raw_predictions\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:678\u001b[0m, in \u001b[0;36mBaseGradientBoosting._raw_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raw_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the sum of the trees raw predictions (+ init estimator).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_predict_init(X)\n\u001b[1;32m    679\u001b[0m     predict_stages(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_, X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate, raw_predictions)\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m raw_predictions\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:664\u001b[0m, in \u001b[0;36mBaseGradientBoosting._raw_predict_init\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raw_predict_init\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    663\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check input and compute raw predictions of the init estimator.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_initialized()\n\u001b[1;32m    665\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_validate_X_predict(X, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:367\u001b[0m, in \u001b[0;36mBaseGradientBoosting._check_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_initialized\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    366\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that the estimator is initialized, raising an error if not.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1462\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[1;32m   1461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[0;32m-> 1462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This GradientBoostingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "#  --- Attempt 1 -----------------------------------------\n",
    "'''\n",
    "# Before realizing the fake class was imbalanced\n",
    "# model = GradientBoostingClassifier()\n",
    "'''\n",
    "# ---- Accuracy and report ---\n",
    "# 0.9989466661985184\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       1.00      1.00      1.00     56864\n",
    "#            1       0.74      0.60      0.66        98\n",
    "\n",
    "#     accuracy                           1.00     56962\n",
    "#    macro avg       0.87      0.80      0.83     56962\n",
    "# weighted avg       1.00      1.00      1.00     56962\n",
    "\n",
    "# -------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# --- Attempt 2 -----------------------------------------\n",
    "'''\n",
    "# After realizing the fake class was imbalanced - using `sample weights`\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "'''\n",
    "# ---- Accuracy and report ---\n",
    "# 0.9927846634598504\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       1.00      0.99      1.00     56864\n",
    "#            1       0.18      0.93      0.31        98\n",
    "\n",
    "#     accuracy                           0.99     56962\n",
    "#    macro avg       0.59      0.96      0.65     56962\n",
    "# weighted avg       1.00      0.99      1.00     56962\n",
    "\n",
    "# -------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# --- Attempt 3 -----------------------------------------\n",
    "\n",
    "\n",
    "# Initialize\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Array of probabilities from the positive class\n",
    "y_proba = model.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "# Adjusting the threshold to optimize precision-recall trade-off\n",
    "threshold = 0.24\n",
    "\n",
    "# Cut out probabilities that do not meet threshold\n",
    "y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "# Report\n",
    "# ---- Accuracy and report ---\n",
    "# 0.9990695551420246\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       1.00      1.00      1.00     56864\n",
    "#            1       0.75      0.69      0.72        98\n",
    "\n",
    "#     accuracy                           1.00     56962\n",
    "#    macro avg       0.87      0.85      0.86     56962\n",
    "# weighted avg       1.00      1.00      1.00     56962\n",
    "\n",
    "# -------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# y_proba = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "\n",
    "# # Adjusting the threshold to optimize precision-recall trade-off\n",
    "# threshold = 0.24  # Example threshold, adjust as needed\n",
    "# y_pred = (y_proba >= threshold).astype(int)\n",
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Accuracy and report ---\n",
      "0.9927846634598504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     56864\n",
      "           1       0.18      0.93      0.31        98\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.59      0.96      0.65     56962\n",
      "weighted avg       1.00      0.99      1.00     56962\n",
      "\n",
      "---- Evaluate if overfitted ---\n",
      "Training Score: 0.9930874059119138\n",
      "Testing Score: 0.9927846634598504\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "print('---- Accuracy and report ---')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(accuracy)\n",
    "print(report)\n",
    "\n",
    "\n",
    "print('---- Evaluate if overfitted ---')\n",
    "train_score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "print(f\"Training Score: {train_score}\")\n",
    "print(f\"Testing Score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cross validation\n",
    "# # Perform cross-validation\n",
    "# cv_scores = cross_val_score(model, X_train, y_train, cv=5)  # cv=5 means 5-fold cross-validation\n",
    "\n",
    "# # Print the cross-validation scores\n",
    "# print(\"Cross-validation scores:\", cv_scores)\n",
    "# print(\"Mean CV accuracy:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export report\n",
    "# with open('report.txt', 'w') as file:\n",
    "#     file.write(report)\n",
    "\n",
    "# Export head\n",
    "# with open('head.txt', 'w') as file:\n",
    "#     file.write(str(dataframe.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model\n",
    "# import pickle\n",
    "# with open('credit_card_fraud_detector.pkl','wb') as f:\n",
    "#     pickle.dump(model,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
